{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Product Review Sentiment Analysis**\n",
    "The goal of this project is to analyse customer reviews to determine their sentiment (positive, negative or neutral) based on the text content of the reviews and associated metadata. This will help in understanding customer feedback, identifying product strengths and weaknesses and improving the overall customer experience by offering actionable insights.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Steps in the Project**\n",
    "\n",
    "1. **Data Collection**: Utilise the provided dataset containing customer reviews and associated metadata, such as star ratings and product categories.\n",
    "2. **Data Preprocessing**: Handle missing data, clean text (e.g remove noise and irrelevant symbols) and standardise review content.\n",
    "3. **Sentiment Labelling**: Use star ratings to label reviews as negative (1-2 stars), neutral (3 stars) or positive (4-5 stars).\n",
    "4. **Exploratory Data Analysis (EDA)**:  Visualise the distribution of sentiments across different product categories and review ratings.\n",
    "5. **Feature Extraction and Text Processing**: Convert review text into numerical features using tokenization, stopword removal and TF-IDF vectorization.\n",
    "6. **Model Selection and Model Training**: Train a machine learning model (such as logistic regression, random forest or neural networks) to predict review sentiment based on extracted features.\n",
    "7. **Model Evaluation**: Assess model performance using accuracy, precision, recall and F1 score to ensure reliable sentiment predictions.\n",
    "8. **Performance Metrics**: Analyse metrics like confusion matrix and detailed performance scores to understand the strengths and weaknesses of the model.\n",
    "9. **Building a Sentiment Analysis Dashboard with Streamlit and Real-Time Sentiment Prediction**: Create an interactive dashboard to visualise sentiment distribution and allow users to input reviews for real-time sentiment predictions.\n",
    "10. **Business Insights and Recommendations**: Provide actionable insights and recommendations to businesses to improve product offerings based on patterns identified in customer feedback."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 1: Importing Required Libraries and Data Collection** ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mThe kernel died. Error: ... View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "import warnings\n",
    "from collections import Counter\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset with the correct delimiter\n",
    "file_path = \"Amazon Product Review.txt\"\n",
    "data = pd.read_csv(file_path, delimiter=\",\", encoding=\"utf-8\")\n",
    "# Rename columns if necessary\n",
    "data.columns = data.columns.str.strip()  # Remove any leading/trailing spaces\n",
    "\n",
    "# Print dataset info\n",
    "print(\"‚úÖ Dataset loaded successfully!\")\n",
    "print(f\"Shape: {data.shape}\")\n",
    "print(\"\\nüß± Columns:\")\n",
    "print(data.columns.tolist())\n",
    "print(\"\\nüìä Missing values:\")\n",
    "print(data.isnull().sum())\n",
    "\n",
    "# Optional: Preview the data\n",
    "print(\"\\nüîç Sample data:\")\n",
    "print(data.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 2 - Data Preprocessing** ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with missing values in 'review_headline' or 'review_body'\n",
    "data.dropna(subset=['review_headline', 'review_body'], inplace=True)\n",
    "\n",
    "# Function to clean text data\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'<.*?>', '', text)  # Remove HTML tags\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    text = re.sub(r'\\d+', '', text)      # Remove numbers\n",
    "    text = text.lower()                   # Convert to lowercase\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "    return text\n",
    "\n",
    "# Apply the clean_text function to 'review_headline' and 'review_body'\n",
    "data['cleaned_review_headline'] = data['review_headline'].apply(clean_text)\n",
    "data['cleaned_review_body'] = data['review_body'].apply(clean_text)\n",
    "\n",
    "# Clean extra spaces\n",
    "data['cleaned_review_headline'] = data['cleaned_review_headline'].str.replace(r'\\s+', ' ', regex=True).str.strip()\n",
    "data['cleaned_review_body'] = data['cleaned_review_body'].str.replace(r'\\s+', ' ', regex=True).str.strip()\n",
    "\n",
    "# Verify the new columns\n",
    "print(\"Cleaned Review Headline and Body:\")\n",
    "print(data[['cleaned_review_headline', 'cleaned_review_body']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 3 - Sentiment Labelling** ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Sentiment Labelling\n",
    "\n",
    "# Define a mapping from star ratings to sentiment labels\n",
    "sentiment_map = {\n",
    "    1: 'very negative',\n",
    "    2: 'negative',\n",
    "    3: 'neutral',\n",
    "    4: 'positive',\n",
    "    5: 'very positive'\n",
    "}\n",
    "\n",
    "# Apply mapping\n",
    "data['sentiment_label'] = data['star_rating'].map(sentiment_map)\n",
    "\n",
    "# Handle unexpected values\n",
    "data['sentiment_label'].fillna('unknown', inplace=True)\n",
    "\n",
    "# Verify the result\n",
    "print(\"\\n‚úÖ Sentiment labels based on star ratings:\")\n",
    "print(data[['star_rating', 'sentiment_label']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 4 - Exploratory Data Analysis (EDA)** ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the sentiment labels for each star rating\n",
    "sentiment_counts_by_rating = (\n",
    "    data.groupby('star_rating')['sentiment_label']\n",
    "    .value_counts()\n",
    "    .unstack(fill_value=0)\n",
    "    .reindex(columns=['very negative', 'negative', 'neutral', 'positive', 'very positive'])\n",
    ")\n",
    "\n",
    "# Display the result\n",
    "print(\"\\n‚úÖ Sentiment Counts by Star Rating:\")\n",
    "print(sentiment_counts_by_rating)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the correct order for sentiment categories\n",
    "sentiment_order = ['very negative', 'negative', 'neutral', 'positive', 'very positive']\n",
    "\n",
    "# Reindex the columns of sentiment_counts_by_rating to match the desired order\n",
    "sentiment_counts_by_rating = sentiment_counts_by_rating.reindex(columns=sentiment_order)\n",
    "\n",
    "# Define a custom color palette for 5 categories\n",
    "custom_colors = ['#B22222',  # very negative (dark red)\n",
    "                 '#FF6347',  # negative (tomato red)\n",
    "                 '#FFD700',  # neutral (gold)\n",
    "                 '#32CD32',  # positive (lime green)\n",
    "                 '#006400']  # very positive (dark green)\n",
    "\n",
    "# Plot the distribution of sentiment across star ratings\n",
    "plt.figure(figsize=(16, 7))\n",
    "sentiment_counts_by_rating.plot(\n",
    "    kind='bar',\n",
    "    stacked=True,\n",
    "    color=custom_colors,\n",
    "    edgecolor='black'\n",
    ")\n",
    "plt.title('Sentiment Distribution Across Star Ratings', fontsize=16, color='white')\n",
    "plt.xlabel('Star Rating', fontsize=14, color='lightgray')\n",
    "plt.ylabel('Number of Reviews', fontsize=14, color='lightgray')\n",
    "plt.xticks(rotation=0, color='lightgray')\n",
    "plt.yticks(color='lightgray')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.legend(\n",
    "    title='Sentiment',\n",
    "    fontsize=12,\n",
    "    bbox_to_anchor=(1, 1),\n",
    "    loc='upper left',\n",
    "    frameon=False\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the correct order for sentiment categories\n",
    "sentiment_order = ['very negative', 'negative', 'neutral', 'positive', 'very positive']\n",
    "\n",
    "# Group and reindex to ensure correct order\n",
    "sentiment_counts_by_category = data.groupby('product_category')['sentiment_label'].value_counts().unstack().fillna(0)\n",
    "sentiment_counts_by_category = sentiment_counts_by_category.reindex(columns=sentiment_order)\n",
    "\n",
    "# Use the same custom colors as before\n",
    "custom_colors = ['#B22222',  # very negative (dark red)\n",
    "                 '#FF6347',  # negative (tomato red)\n",
    "                 '#FFD700',  # neutral (gold)\n",
    "                 '#32CD32',  # positive (lime green)\n",
    "                 '#006400']  # very positive (dark green)\n",
    "\n",
    "plt.style.use('dark_background')  # Set dark background\n",
    "plt.figure(figsize=(18, 8))\n",
    "sentiment_counts_by_category.plot(\n",
    "    kind='bar',\n",
    "    stacked=True,\n",
    "    color=custom_colors,\n",
    "    edgecolor='black'\n",
    ")\n",
    "plt.title('Sentiment Distribution Across Product Categories', fontsize=16, color='white')\n",
    "plt.xlabel('Product Category', fontsize=14, color='lightgray')\n",
    "plt.ylabel('Number of Reviews', fontsize=14, color='lightgray')\n",
    "plt.xticks(rotation=45, ha='right', color='lightgray')\n",
    "plt.yticks(color='lightgray')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.legend(title='Sentiment', fontsize=12, bbox_to_anchor=(1, 1), loc='upper left', frameon=False)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Star Rating Distribution by Sentiment\n",
    "# Count plot for sentiment distribution across star ratings\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(data=data, x='star_rating', hue='sentiment_label', palette='Set2')\n",
    "plt.title('Sentiment Distribution Across Star Ratings', fontsize=18)\n",
    "plt.xlabel('Star Rating', fontsize=14)\n",
    "plt.ylabel('Number of Reviews', fontsize=14)\n",
    "plt.legend(title='Sentiment', fontsize=12)\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Perform Basic Text Analysis - Word Cloud\n",
    "# Combine all cleaned reviews into a single string\n",
    "all_reviews = ' '.join(data['cleaned_review_body'])\n",
    "\n",
    "# Generate a word cloud with a unique color scheme\n",
    "wordcloud = WordCloud(\n",
    "    width=800,\n",
    "    height=400,\n",
    "    background_color='black',\n",
    "    colormap='Spectral',\n",
    "    contour_color='white',\n",
    "    contour_width=1,\n",
    "    random_state=0  # Ensures reproducibility\n",
    ").generate(all_reviews)\n",
    "\n",
    "# Display the word cloud\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')  # Hide axes\n",
    "plt.title('Word Cloud of Reviews', fontsize=16, color='white')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Most Common Words\n",
    "# Tokenize the cleaned review body\n",
    "all_words = ' '.join(data['cleaned_review_body']).split()\n",
    "\n",
    "# Count the frequency of each word\n",
    "word_counts = Counter(all_words)\n",
    "\n",
    "# Get the most common words (e.g., top 20)\n",
    "most_common_words = word_counts.most_common(20)\n",
    "\n",
    "# Create a DataFrame from the most common words\n",
    "common_words_df = pd.DataFrame(most_common_words, columns=['Word', 'Frequency'])\n",
    "\n",
    "# Plot the most common words with a vibrant color palette and custom styling\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='Frequency', y='Word', data=common_words_df, hue='Word', palette='magma', errorbar=None, legend=False)\n",
    "plt.title('Most Common Words in Reviews', fontsize=18, fontweight='bold', color='white')\n",
    "plt.xlabel('Frequency', fontsize=14, color='lightgray')\n",
    "plt.ylabel('Words', fontsize=14, color='lightgray')\n",
    "plt.grid(axis='x', color='gray', linestyle='--', linewidth=0.7)\n",
    "plt.xticks(color='lightgray')\n",
    "plt.yticks(color='lightgray')\n",
    "plt.gca().set_facecolor('black')  # Set the background color to black\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 5: Feature Extraction and Text Processing** ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean text data\n",
    "def clean_text(text):\n",
    "    # Remove HTML tags and special characters\n",
    "    text = re.sub(r'<.*?>', '', text)  # Remove HTML tags\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    text = re.sub(r'\\d+', '', text)      # Remove numbers\n",
    "    text = text.lower()                   # Convert to lowercase\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Apply the clean_text function to 'review_body'\n",
    "data['cleaned_review_body'] = data['review_body'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map sentiment_label once\n",
    "sentiment_map = {\n",
    "    'very negative': 0,\n",
    "    'negative': 1,\n",
    "    'neutral': 2,\n",
    "    'positive': 3,\n",
    "    'very positive': 4\n",
    "}\n",
    "data['sentiment_code'] = data['sentiment_label'].map(sentiment_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improved TF-IDF vectorizer (fit once)\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=10000,\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=5,\n",
    "    max_df=0.9,\n",
    "    stop_words='english'\n",
    ")\n",
    "X = vectorizer.fit_transform(data['cleaned_review_body'])\n",
    "y = data['sentiment_code']\n",
    "\n",
    "# Optional: preview TF-IDF matrix and words with \"love\"\n",
    "tfidf_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "print(\"Shape of TF-IDF matrix:\", X.shape)\n",
    "\n",
    "k_words = [word for word in tfidf_df.columns if word.startswith('love')]\n",
    "tfidf_k_df = tfidf_df[k_words]\n",
    "print(\"TF-IDF features starting with 'love':\\n\", tfidf_k_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 6: Model Selection and Model Training** ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "from sklearn.utils import resample\n",
    "\n",
    "results = {}\n",
    "# Split the data into training and test sets with stratified sampling\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=0, stratify=y\n",
    ")\n",
    "\n",
    "# Apply SMOTE oversampling on training data only\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# 1. Logistic Regression\n",
    "lr_model = LogisticRegression(max_iter=1000)\n",
    "lr_model.fit(X_train_balanced, y_train_balanced)\n",
    "y_pred_lr = lr_model.predict(X_test)\n",
    "results['Logistic Regression'] = {\n",
    "    'accuracy': accuracy_score(y_test, y_pred_lr),\n",
    "    'precision': precision_score(y_test, y_pred_lr, average='weighted'),\n",
    "    'recall': recall_score(y_test, y_pred_lr, average='weighted'),\n",
    "    'f1': f1_score(y_test, y_pred_lr, average='weighted'),\n",
    "    'report': classification_report(y_test, y_pred_lr)\n",
    "}\n",
    "\n",
    "# 2. Multinomial Naive Bayes\n",
    "nb_model = MultinomialNB()\n",
    "nb_model.fit(X_train_balanced, y_train_balanced)\n",
    "y_pred_nb = nb_model.predict(X_test)\n",
    "results['Naive Bayes'] = {\n",
    "    'accuracy': accuracy_score(y_test, y_pred_nb),\n",
    "    'precision': precision_score(y_test, y_pred_nb, average='weighted'),\n",
    "    'recall': recall_score(y_test, y_pred_nb, average='weighted'),\n",
    "    'f1': f1_score(y_test, y_pred_nb, average='weighted'),\n",
    "    'report': classification_report(y_test, y_pred_nb)\n",
    "}\n",
    "\n",
    "# 3. Linear SVM\n",
    "svm_model = LinearSVC(max_iter=1000)\n",
    "svm_model.fit(X_train_balanced, y_train_balanced)\n",
    "y_pred_svm = svm_model.predict(X_test)\n",
    "results['SVM'] = {\n",
    "    'accuracy': accuracy_score(y_test, y_pred_svm),\n",
    "    'precision': precision_score(y_test, y_pred_svm, average='weighted'),\n",
    "    'recall': recall_score(y_test, y_pred_svm, average='weighted'),\n",
    "    'f1': f1_score(y_test, y_pred_svm, average='weighted'),\n",
    "    'report': classification_report(y_test, y_pred_svm)\n",
    "}\n",
    "\n",
    "# Optionally, print results summary\n",
    "for model_name, metrics in results.items():\n",
    "    print(f\"=== {model_name} ===\")\n",
    "    print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n",
    "    print(f\"Precision: {metrics['precision']:.4f}\")\n",
    "    print(f\"Recall: {metrics['recall']:.4f}\")\n",
    "    print(f\"F1-score: {metrics['f1']:.4f}\")\n",
    "    print(metrics['report'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 7: Model Evaluation** ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After training and predicting with all models and storing results in `results` dictionary:\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Extract key metrics into a DataFrame for easy comparison\n",
    "comparison_df = pd.DataFrame({\n",
    "    model: {\n",
    "        'Accuracy': metrics['accuracy'],\n",
    "        'Precision': metrics['precision'],\n",
    "        'Recall': metrics['recall'],\n",
    "        'F1 Score': metrics['f1']\n",
    "    } for model, metrics in results.items()\n",
    "}).T  # Transpose to have models as rows\n",
    "\n",
    "print(\"Model Performance Comparison:\\n\")\n",
    "print(comparison_df)\n",
    "\n",
    "# Find the best model by F1 score\n",
    "best_model = comparison_df['F1 Score'].idxmax()\n",
    "best_f1 = comparison_df.loc[best_model, 'F1 Score']\n",
    "\n",
    "print(f\"\\nBest model based on F1 Score: {best_model} (F1 Score = {best_f1:.4f})\")\n",
    "\n",
    "# Optionally, print the classification report for the best model\n",
    "print(f\"\\nClassification Report for {best_model}:\\n\")\n",
    "print(results[best_model]['report'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the confusion matrix for Naive Bayes\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_nb)\n",
    "\n",
    "# Define the class labels for display\n",
    "labels = ['Very Neg', 'Neg', 'Neutral', 'Pos', 'Very Pos']\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=labels, yticklabels=labels)\n",
    "\n",
    "plt.title('Confusion Matrix - Naive Bayes')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 8: Performance Metrics** ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Check if the model is a Naive Bayes model\n",
    "if hasattr(nb_model, 'feature_log_prob_'):\n",
    "    # Get feature names from the vectorizer\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "    # Get log probabilities (shape: n_classes x n_features)\n",
    "    class_labels = nb_model.classes_\n",
    "    log_probs = nb_model.feature_log_prob_\n",
    "\n",
    "    # Calculate the difference between \"very positive\" and \"very negative\"\n",
    "    class_index_positive = np.where(class_labels == 4)[0][0]  # 'very positive'\n",
    "    class_index_negative = np.where(class_labels == 0)[0][0]  # 'very negative'\n",
    "\n",
    "    # Calculate the difference in log-probability\n",
    "    importance = log_probs[class_index_positive] - log_probs[class_index_negative]\n",
    "\n",
    "    # Create a DataFrame for feature importance\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': importance\n",
    "    })\n",
    "\n",
    "    # Sort by absolute importance\n",
    "    importance_df['AbsImportance'] = np.abs(importance_df['Importance'])\n",
    "    importance_df = importance_df.sort_values(by='AbsImportance', ascending=False)\n",
    "\n",
    "    # Display top 10 most important features\n",
    "    print(\"Top 10 most important features for classifying sentiment (very pos vs. very neg):\")\n",
    "    print(importance_df.head(10))\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(data=importance_df.head(10), x='Importance', y='Feature', palette='coolwarm')\n",
    "    plt.title('Top 10 Discriminative Features (Very Positive vs Very Negative)')\n",
    "    plt.xlabel('Log Probability Difference')\n",
    "    plt.ylabel('Feature')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"The model does not have feature_log_prob_ (maybe not a Naive Bayes model).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save both model and vectorizer\n",
    "joblib.dump(nb_model, 'sentiment_model.pkl')\n",
    "joblib.dump(vectorizer, 'vectorizer.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
